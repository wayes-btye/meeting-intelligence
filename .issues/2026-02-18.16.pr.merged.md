# [\#16 PR](https://github.com/wayes-btye/meeting-intelligence/pull/16) `merged`: feat: evaluation framework â€” metrics, cross-check, strategy comparison (#7)

#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-18 17:20](https://github.com/wayes-btye/meeting-intelligence/pull/16):

## Summary
- Auto-generate Q&A test sets from meeting transcripts using Claude (150-250 questions across categories and difficulty levels)
- RAGAS-style metrics via Claude-as-judge: faithfulness, answer relevancy, context precision, context recall
- Cross-check evaluation: RAG vs context-stuffing comparison with verdict categorization
- Strategy comparison across all 4 combinations (naive/speaker_turn Ã— semantic/hybrid) with markdown report
- Evaluation runner orchestrates full pipeline and generates markdown + JSON reports
- 31 new tests, all passing

## Test plan
- [x] `pytest tests/test_evaluation.py -v` â€” all 31 tests pass
- [x] Metric score clamping and edge cases tested
- [x] Cross-check summarization with per-category breakdowns tested
- [x] Report generation with strategy comparison tables tested

Closes #7

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)




-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.03.01 at 04:23:01.]
