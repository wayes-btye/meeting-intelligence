# [\#20 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/20) `open`: feat: LLM routing layer — replace direct Anthropic SDK with LiteLLM for model flexibility
**Labels**: `enhancement`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-18 21:49](https://github.com/wayes-btye/meeting-intelligence/issues/20):

## Problem

We're currently hardcoded to Claude via the Anthropic SDK for three functions:
1. **Answer generation** (`src/retrieval/generation.py`) — Claude Sonnet
2. **Structured extraction** (`src/extraction/extractor.py`) — Claude with `tool_use`
3. **Evaluation metrics** (`src/evaluation/`) — Claude as judge

This means we can't test whether GPT-4o, Gemini, or open-source models (Llama, Mistral) perform better or worse at any of these tasks. For the strategy toggle system to be truly useful, we should be able to swap LLMs the same way we swap chunking and retrieval strategies.

## Proposed Solution: LiteLLM

After researching the options:

| Option | Pros | Cons |
|--------|------|------|
| **OpenRouter** | 500+ models, managed, single billing | 5% markup, SaaS dependency, tool_use inconsistent across models |
| **LiteLLM** | Open-source, self-hosted, 100+ providers, unified API for both LLM + embeddings | Requires local proxy setup |
| **Portkey** | 1600+ models, enterprise compliance | $49/mo, overkill for this project |

**Recommendation: LiteLLM** because:
- It's open-source and free (no markup)
- Single `pip install litellm` — no external proxy needed for basic use
- Unified API covers **both** chat completions AND embeddings (relevant to #11)
- Supports tool_use/function calling across Claude, GPT-4, Gemini
- Can be used as a simple Python library (no proxy server needed)

## Implementation Plan

### 1. Add LiteLLM dependency
```toml
# pyproject.toml
"litellm>=1.0"
```

### 2. Add model config to Settings
```python
# src/config.py
class Settings(BaseSettings):
    llm_model: str = "anthropic/claude-sonnet-4-20250514"  # LiteLLM format
    eval_model: str = "anthropic/claude-sonnet-4-20250514"
    extraction_model: str = "anthropic/claude-sonnet-4-20250514"
```

### 3. Replace Anthropic SDK calls with LiteLLM
```python
# Before (generation.py)
from anthropic import Anthropic
client = Anthropic()
response = client.messages.create(model="claude-sonnet-4-20250514", ...)

# After
import litellm
response = litellm.completion(model=settings.llm_model, messages=[...])
```

### 4. Update extraction to use LiteLLM tool_use
```python
# Before (extractor.py) 
response = client.messages.create(tools=[...], tool_choice={...})

# After
response = litellm.completion(model=settings.extraction_model, tools=[...], tool_choice={...})
```

### 5. Add LLM model to strategy toggle UI
- Sidebar dropdown for model selection (Claude Sonnet, GPT-4o, Gemini 2.0, etc.)
- Separate model selectors for generation vs extraction vs evaluation

### 6. Add to PipelineConfig
```python
@dataclass(frozen=True)
class PipelineConfig:
    chunking_strategy: ChunkingStrategy = ChunkingStrategy.SPEAKER_TURN
    retrieval_strategy: RetrievalStrategy = RetrievalStrategy.HYBRID
    llm_model: str = "anthropic/claude-sonnet-4-20250514"
```

## Compatibility Concerns

### Tool use across models
- **Claude**: Native `tool_use` — works perfectly
- **GPT-4o/4-turbo**: OpenAI `function_calling` — LiteLLM translates automatically
- **Gemini 2.0**: Supports function calling — LiteLLM handles translation
- **Open-source (Llama, Mistral)**: Tool use support is **inconsistent**. Some models claim support but produce malformed JSON. Extraction may fail with these.

### Mitigation
- Extraction (which requires reliable structured output) should default to Claude or GPT-4o
- Generation (free-form text) can use any model
- Evaluation (Claude-as-judge) could be compared across judge models — interesting meta-evaluation

### API key management
With LiteLLM, each provider needs its own key:
```env
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
```
LiteLLM reads these automatically from env vars.

## Acceptance Criteria
- [ ] `litellm` added as dependency
- [ ] All three call sites (generation, extraction, evaluation) use LiteLLM
- [ ] Model is configurable via Settings / PipelineConfig
- [ ] Streamlit sidebar has model selector
- [ ] Tests pass with mocked LiteLLM calls
- [ ] Can switch to GPT-4o and get valid answers (manual test)




-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.02.23 at 04:25:17.]
