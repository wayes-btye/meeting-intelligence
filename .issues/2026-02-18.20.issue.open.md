# [\#20 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/20) `open`: feat: LLM routing layer — replace direct Anthropic SDK with LiteLLM for model flexibility
**Labels**: `enhancement`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-18 21:49](https://github.com/wayes-btye/meeting-intelligence/issues/20):

## Problem

We're currently hardcoded to Claude via the Anthropic SDK for three functions:
1. **Answer generation** (`src/retrieval/generation.py`) — Claude Sonnet
2. **Structured extraction** (`src/extraction/extractor.py`) — Claude with `tool_use`
3. **Evaluation metrics** (`src/evaluation/`) — Claude as judge

This means we can't test whether GPT-4o, Gemini, or open-source models (Llama, Mistral) perform better or worse at any of these tasks. For the strategy toggle system to be truly useful, we should be able to swap LLMs the same way we swap chunking and retrieval strategies.

## Proposed Solution: LiteLLM

After researching the options:

| Option | Pros | Cons |
|--------|------|------|
| **OpenRouter** | 500+ models, managed, single billing | 5% markup, SaaS dependency, tool_use inconsistent across models |
| **LiteLLM** | Open-source, self-hosted, 100+ providers, unified API for both LLM + embeddings | Requires local proxy setup |
| **Portkey** | 1600+ models, enterprise compliance | $49/mo, overkill for this project |

**Recommendation: LiteLLM** because:
- It's open-source and free (no markup)
- Single `pip install litellm` — no external proxy needed for basic use
- Unified API covers **both** chat completions AND embeddings (relevant to #11)
- Supports tool_use/function calling across Claude, GPT-4, Gemini
- Can be used as a simple Python library (no proxy server needed)

## Implementation Plan

### 1. Add LiteLLM dependency
```toml
# pyproject.toml
"litellm>=1.0"
```

### 2. Add model config to Settings
```python
# src/config.py
class Settings(BaseSettings):
    llm_model: str = "anthropic/claude-sonnet-4-20250514"  # LiteLLM format
    eval_model: str = "anthropic/claude-sonnet-4-20250514"
    extraction_model: str = "anthropic/claude-sonnet-4-20250514"
```

### 3. Replace Anthropic SDK calls with LiteLLM
```python
# Before (generation.py)
from anthropic import Anthropic
client = Anthropic()
response = client.messages.create(model="claude-sonnet-4-20250514", ...)

# After
import litellm
response = litellm.completion(model=settings.llm_model, messages=[...])
```

### 4. Update extraction to use LiteLLM tool_use
```python
# Before (extractor.py) 
response = client.messages.create(tools=[...], tool_choice={...})

# After
response = litellm.completion(model=settings.extraction_model, tools=[...], tool_choice={...})
```

### 5. Add LLM model to strategy toggle UI
- Sidebar dropdown for model selection (Claude Sonnet, GPT-4o, Gemini 2.0, etc.)
- Separate model selectors for generation vs extraction vs evaluation

### 6. Add to PipelineConfig
```python
@dataclass(frozen=True)
class PipelineConfig:
    chunking_strategy: ChunkingStrategy = ChunkingStrategy.SPEAKER_TURN
    retrieval_strategy: RetrievalStrategy = RetrievalStrategy.HYBRID
    llm_model: str = "anthropic/claude-sonnet-4-20250514"
```

## Compatibility Concerns

### Tool use across models
- **Claude**: Native `tool_use` — works perfectly
- **GPT-4o/4-turbo**: OpenAI `function_calling` — LiteLLM translates automatically
- **Gemini 2.0**: Supports function calling — LiteLLM handles translation
- **Open-source (Llama, Mistral)**: Tool use support is **inconsistent**. Some models claim support but produce malformed JSON. Extraction may fail with these.

### Mitigation
- Extraction (which requires reliable structured output) should default to Claude or GPT-4o
- Generation (free-form text) can use any model
- Evaluation (Claude-as-judge) could be compared across judge models — interesting meta-evaluation

### API key management
With LiteLLM, each provider needs its own key:
```env
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
```
LiteLLM reads these automatically from env vars.

## Acceptance Criteria
- [ ] `litellm` added as dependency
- [ ] All three call sites (generation, extraction, evaluation) use LiteLLM
- [ ] Model is configurable via Settings / PipelineConfig
- [ ] Streamlit sidebar has model selector
- [ ] Tests pass with mocked LiteLLM calls
- [ ] Can switch to GPT-4o and get valid answers (manual test)

#### <img src="https://avatars.githubusercontent.com/u/136032180?v=4" width="50">[hshahrokni2](https://github.com/hshahrokni2) commented at [2026-02-23 11:47](https://github.com/wayes-btye/meeting-intelligence/issues/20#issuecomment-3944293584):

Good analysis. The LiteLLM vs OpenRouter tradeoff you laid out is accurate, though one thing worth adding: LiteLLM's local library mode (no proxy) is much simpler than running the full proxy stack, and for this use case (testing model swaps) it's probably sufficient.

One practical note on your compatibility table: tool_use reliability across models has improved since this was written, but Gemini structured output still has edge cases. If extraction correctness matters a lot, keeping Claude or GPT-4o as the extraction model and only swapping generation makes the experiment cleaner.

On the API key management section — worth knowing there's a third option: use an OpenAI-compatible routing endpoint that holds all provider keys and exposes a single . You call  and it routes to Anthropic; call a generic tier and it picks based on cost/capability. We've used Komilion (komilion.com) for this on projects where we wanted to benchmark across providers without managing per-provider credentials locally. Returns cost per request in the body which made the benchmarking side straightforward.

The split between generation, extraction, and evaluation models is good — most people don't think to use different models for each. Worth adding that evaluation (Claude-as-judge) has known self-serving biases when evaluating Claude outputs; using a different family for evaluation is actually more rigorous.


-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.02.24 at 04:20:21.]
