# [\#21 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/21) `open`: feat: configurable embedding models with migration support
**Labels**: `enhancement`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-18 21:50](https://github.com/wayes-btye/meeting-intelligence/issues/21):

## Problem

We're locked to OpenAI `text-embedding-3-small` (1536 dimensions). We can't test whether Cohere, Voyage AI, or other embedding models produce better retrieval results — which is arguably more impactful than swapping LLMs, since retrieval quality is the bottleneck in RAG.

## The Compatibility Problem (this is the hard part)

Changing embedding models is **not** like swapping LLMs. Here's why:

### 1. Different models = different vector spaces
Even if two models both output 1536 dimensions, the vectors are **incompatible**. You cannot mix embeddings from different models in the same search — cosine similarity between vectors from different models is meaningless.

### 2. Dimension changes break the schema
Our Supabase schema is hardcoded:
\`\`\`sql
embedding halfvec(1536)              -- fixed dimension
CREATE INDEX chunks_embedding_idx ON chunks
USING hnsw (embedding halfvec_cosine_ops);  -- index is dimension-specific
\`\`\`

Changing to a model with different dimensions (e.g., Cohere embed-v3 at 1024, or text-embedding-3-large at 3072) requires:
- ALTER the column type
- DROP and rebuild the HNSW index
- Re-embed ALL existing chunks

### 3. pgvector dimension limits
pgvector caps at ~2000 dimensions for indexed vectors (8KB block limit). Models like `text-embedding-3-large` (3072 dims) need dimension reduction via the API's \`dimensions\` parameter or won't work with HNSW indexing.

### 4. Re-embedding is expensive
With 50 meetings × ~60 chunks each = ~3,000 chunks. Re-embedding all of them costs:
- OpenAI: ~$0.01 (cheap)
- Cohere: ~$0.01 (cheap)
- Voyage AI: ~$0.02 (cheap)

Actually not bad for our dataset size. The real cost is time and complexity.

## Proposed Solution: LiteLLM Embeddings + Metadata Tracking

### Approach: Use LiteLLM for embeddings too (pairs with #20)

LiteLLM supports embeddings from all major providers with a unified API:
\`\`\`python
import litellm

# OpenAI
response = litellm.embedding(model="text-embedding-3-small", input=["text"])

# Cohere  
response = litellm.embedding(model="cohere/embed-english-v3.0", input=["text"])

# Voyage AI
response = litellm.embedding(model="voyage/voyage-3", input=["text"])
\`\`\`

### Implementation Plan

#### 1. Add embedding model to Settings + PipelineConfig
\`\`\`python
class Settings(BaseSettings):
    embedding_model: str = "text-embedding-3-small"
    embedding_dimensions: int = 1536
\`\`\`

#### 2. Replace direct OpenAI calls with LiteLLM
\`\`\`python
# Before (embeddings.py)
from openai import OpenAI
client = OpenAI()
response = client.embeddings.create(model="text-embedding-3-small", input=texts)

# After
import litellm
response = litellm.embedding(model=settings.embedding_model, input=texts)
\`\`\`

#### 3. Track embedding model per chunk
Add \`embedding_model\` column to chunks table:
\`\`\`sql
ALTER TABLE chunks ADD COLUMN embedding_model TEXT DEFAULT 'text-embedding-3-small';
\`\`\`

This lets us know which model produced each vector, critical for:
- Filtering search to only match chunks with the same embedding model
- Knowing when re-embedding is needed

#### 4. Handle dimension changes via migration script
\`\`\`python
# scripts/re_embed.py
# Re-embeds all chunks with a new model
# Steps: 
#   1. Alter column dimension if needed
#   2. Drop HNSW index
#   3. Re-embed all chunks in batches
#   4. Rebuild HNSW index
\`\`\`

#### 5. Update search to filter by embedding model
\`\`\`sql
-- match_chunks() should filter: WHERE embedding_model = p_embedding_model
\`\`\`

This prevents accidentally comparing vectors from different models.

#### 6. Side-by-side comparison (stretch goal)
For the evaluation framework, the ideal approach is:
- Ingest the same meetings with two different embedding models (stored with different \`embedding_model\` values)
- If dimensions differ, use separate columns or a second table
- Run the same eval questions against both and compare retrieval metrics

**For same-dimension models** (e.g., text-embedding-3-small vs Cohere embed-v3 with \`dimensions=1536\`): Can coexist in the same column if search filters by \`embedding_model\`.

**For different-dimension models**: Would need either:
- A separate \`embedding_alt halfvec(1024)\` column (messy)
- A separate table (cleaner but more complex)
- Just re-embed and compare sequentially (simplest — recommended for MVP)

## Embedding Models Worth Testing

| Model | Provider | Dimensions | Notes |
|-------|----------|-----------|-------|
| text-embedding-3-small | OpenAI | 1536 | Current default, good baseline |
| text-embedding-3-large | OpenAI | 3072 (or 1536 via API) | Can request 1536 dims via \`dimensions\` param |
| embed-english-v3.0 | Cohere | 1024 | Strong on retrieval benchmarks |
| voyage-3 | Voyage AI | 1024 | Top MTEB scores, designed for RAG |
| e5-mistral-7b-instruct | HuggingFace | 4096 | Open-source, very high quality but large |

**Note on Matryoshka embeddings**: OpenAI's v3 models support requesting fewer dimensions (e.g., 512 or 256 from a 1536-dim model). This is useful for testing dimension vs quality trade-offs without changing models.

## Compatibility Matrix

| Change | Schema impact | Re-embed needed? | Index rebuild? |
|--------|-------------|-------------------|----------------|
| Same model, same dims | None | No | No |
| Different model, same dims (1536) | Add model tracking | Yes, all chunks | No (same index works) |
| Different model, different dims | ALTER column + model tracking | Yes, all chunks | Yes (drop + rebuild HNSW) |
| Same model, reduced dims (Matryoshka) | ALTER column | Yes, all chunks | Yes |

## Acceptance Criteria
- [ ] Embedding model configurable via Settings
- [ ] LiteLLM used for embedding calls (unified API)
- [ ] \`embedding_model\` column added to chunks table
- [ ] Search filters by embedding model to prevent cross-model comparison
- [ ] \`scripts/re_embed.py\` migration script for model changes
- [ ] Evaluation can compare retrieval quality across embedding models
- [ ] Streamlit sidebar has embedding model selector

## Depends on
- #20 (LiteLLM integration — shared dependency)




-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.02.21 at 04:07:53.]
