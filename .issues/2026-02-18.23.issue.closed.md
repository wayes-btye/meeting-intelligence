# [\#23 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/23) `closed`: Fix evaluation runner entry point and update README claims
**Labels**: `bug`, `evaluation`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-18 22:23](https://github.com/wayes-btye/meeting-intelligence/issues/23):

Multiple evaluation-related gaps:

- `python -m src.evaluation.runner` has no `__main__` block — the documented command fails
- README claims "RAGAS + DeepEval metrics" but neither library is used — implementation is Claude-as-judge (which is actually a good approach, but the claim is misleading)
- No evaluation results have ever been generated
- No `scripts/run_evaluation.py` CLI script

**Needs:**
- Add `__main__` block to `runner.py`
- Update README to accurately describe Claude-as-judge approach
- Consider generating actual evaluation results




-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.03.01 at 04:23:01.]
