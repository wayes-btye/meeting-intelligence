# [\#7 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/7) `closed`: Evaluation framework
**Labels**: `evaluation`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-18 12:00](https://github.com/wayes-btye/meeting-intelligence/issues/7):

## Description
Auto-generate test set from MeetingBank reference summaries using Claude. Run RAGAS/DeepEval metrics. Cross-check: RAG vs context-stuffing comparison. Strategy comparison report.

## Tasks
- [ ] Auto-generate Q&A test set from MeetingBank reference summaries using Claude
- [ ] Implement RAGAS metrics: faithfulness, answer relevancy, context precision/recall
- [ ] DeepEval integration for additional metrics
- [ ] Cross-check evaluation: run each question through RAG and context-stuffing
- [ ] Categorize disagreements (RAG better, context-stuffing better, equivalent)
- [ ] Strategy comparison: metrics for each chunking x retrieval combination
- [ ] Generate evaluation report with tables and analysis

## Acceptance Criteria
- Evaluation report with metrics for each strategy combination
- Cross-check results showing where RAG adds value vs where it doesn't
- 150-250 test questions across difficulty levels

#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) commented at [2026-02-18 17:20](https://github.com/wayes-btye/meeting-intelligence/issues/7#issuecomment-3922110268):

Completed in PR #16 (merged). Implemented:
- Test set generation using Claude (150-250 Q&A pairs across categories/difficulty)
- RAGAS-style metrics via Claude-as-judge: faithfulness, answer relevancy, context precision/recall
- Cross-check: RAG vs context-stuffing comparison with verdict categorization
- Strategy comparison across all 4 chunking×retrieval combinations
- Evaluation runner with markdown + JSON report generation
- 31 new tests, all passing

#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) commented at [2026-02-18 22:23](https://github.com/wayes-btye/meeting-intelligence/issues/7#issuecomment-3923527261):

**Post-merge audit finding:**

The evaluation framework was built but never successfully run. Three specific problems:

1. **Entry point is missing.** `python -m src.evaluation.runner` (documented in the README) fails because `src/evaluation/runner.py` has no `if __name__ == "__main__":` block and there is no `src/evaluation/__main__.py`. The documented command produces: `No module named src.evaluation.runner.__main__`.

2. **README claim about RAGAS + DeepEval is inaccurate.** The README states "RAGAS + DeepEval metrics" but neither library is installed (`deepeval` is not in `pyproject.toml`) or used anywhere. The actual implementation is a custom Claude-as-judge approach in `src/evaluation/metrics.py`. This is a valid approach, but claiming RAGAS/DeepEval integration is misleading.

3. **No evaluation results were ever generated.** No `reports/` directory exists, no `data/eval_results/` directory exists, no test set (`data/test_set.json`) was generated. The acceptance criteria required "Evaluation report with metrics for each strategy combination" and "Cross-check results" — neither was produced.

The framework code is well-structured but was never run. The entry point is missing and the README claims about RAGAS/DeepEval are inaccurate.

This needs to be addressed in a follow-up issue.


-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.02.27 at 04:14:01.]
