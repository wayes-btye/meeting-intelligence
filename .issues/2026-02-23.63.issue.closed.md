# [\#63 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/63) `closed`: fix: enable speaker diarization in AssemblyAI transcription
**Labels**: `bug`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-23 05:49](https://github.com/wayes-btye/meeting-intelligence/issues/63):

## Problem

When audio files are uploaded, AssemblyAI transcribes them as a single flat block of text with no speaker attribution. This breaks speaker-turn chunking — the whole meeting ends up as ~4 large undifferentiated chunks instead of per-speaker turns.

**Root cause — two bugs in `src/api/routes/ingest.py`:**

**1. `speaker_labels=True` not passed to AssemblyAI**

```python
# Current (broken)
config = aai.TranscriptionConfig(speech_models=["universal-3-pro"])

# Fix
config = aai.TranscriptionConfig(speech_models=["universal-3-pro"], speaker_labels=True)
```

**2. Returning plain text instead of structured utterances**

```python
# Current — discards all speaker/timing data
return transcript.text or ""
# ...
transcript_format = "text"  # routes to parse_plain_text — no speaker extraction
```

The fix should return JSON built from `transcript.utterances` and set `transcript_format = "json"`, which routes to `parse_json`. That parser already handles the AssemblyAI `{"utterances": [...]}` format correctly (see `parsers.py:128`).

## Evidence

Manually verified with `tests/transcribe_sample.py`:
- With `speaker_labels=True`: **7 speakers detected, 88 utterances** on a 24-min meeting
- Without: 1 speaker, 1 segment, 4 chunks

## Fix

In `_transcribe_audio` (`src/api/routes/ingest.py`):

**1.** Add `speaker_labels=True` to `TranscriptionConfig`

**2.** Replace `return transcript.text or ""` with utterances JSON:
```python
utterances = [
    {"speaker": u.speaker, "text": u.text, "start": u.start, "end": u.end}
    for u in (transcript.utterances or [])
]
return json.dumps({"utterances": utterances})
```

**3.** Change `transcript_format = "text"` to `transcript_format = "json"`

## Impact

- Speaker-turn chunking will produce meaningful per-speaker chunks
- Timestamps and speaker labels will be stored with each chunk
- `num_speakers` will reflect the real speaker count

#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) commented at [2026-02-23 11:29](https://github.com/wayes-btye/meeting-intelligence/issues/63#issuecomment-3944215246):

## Fixed in commit [3874038](https://github.com/wayes-btye/meeting-intelligence/commit/3874038)

Both bugs addressed in the portfolio review pass (2026-02-23):

**Fix 1 — `speaker_labels=True` added to `TranscriptionConfig`:**
```python
config = aai.TranscriptionConfig(
    speech_models=["universal-3-pro"],
    speaker_labels=True,  # ← added
)
```

**Fix 2 — Return utterances JSON instead of flat text:**
```python
# Before (discards all speaker data)
return transcript.text or ""

# After (preserves speaker attribution)
utterances = transcript.utterances or []
return json.dumps({
    "utterances": [
        {"speaker": u.speaker, "text": u.text, "start": u.start, "end": u.end}
        for u in utterances
    ]
})
```

**Fix 3 — `transcript_format` updated to `"json"`** in both the single-file audio path and the zip audio path, so `parse_json()` routes to the utterances parser rather than the plain-text parser.

**Test updated:** `test_zip_audio_with_key_is_ingested` mock return value updated to valid AssemblyAI JSON format.

Evidence for the fix was in `tests/transcribe_sample.py` — with `speaker_labels=True`, the GitLab test audio produces 7 speakers and 88 utterances vs 1 speaker / 4 undifferentiated chunks without it.


-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.02.28 at 03:55:48.]
