# [\#64 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/64) `open`: feat: run evaluation against MeetingBank corpus and commit results
**Labels**: `evaluation`, `data`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-23 11:34](https://github.com/wayes-btye/meeting-intelligence/issues/64):

## Why This Matters

The evaluation framework is fully implemented (Claude-as-judge metrics, cross-check, strategy comparison, runner CLI). What's missing: actual results. Without numbers, the framework is a claim, not evidence.

This is the single most important gap before the system can be presented as a complete RAG implementation.

## What to Run

10 MeetingBank meetings are already loaded into Supabase (as of 2026-02-23). The runner is ready.

**Step 1 — Generate test set:**
```bash
python -m src.evaluation.runner \
    --generate-only \
    --output data/test_set.json
```

**Step 2 — Run full evaluation:**
```bash
python -m src.evaluation.runner \
    --test-set data/test_set.json \
    --output reports/eval_results \
    --strategies naive:semantic naive:hybrid speaker_turn:semantic speaker_turn:hybrid
```

**Step 3 — Commit results:**
```
reports/evaluation_report.md       ← human-readable markdown, readable in GitHub
reports/strategy_results.json      ← per-question results by strategy
reports/cross_check_results.json   ← RAG vs context-stuffing comparison
```

## What We Expect to Learn

- Whether speaker-turn chunking outperforms naive for attribution queries ("what did X say about Y?")
- Whether hybrid retrieval outperforms semantic-only for keyword-heavy queries (proper nouns, project names)
- Whether RAG outperforms context-stuffing at 10-meeting scale (hypothesis: marginal; clear win at 50+)
- Which hybrid weight setting (70/30 default) performs best — or whether it needs tuning (see Issue for weight grid-search)

## Cost Estimate

~$5–15 in API calls (Claude-as-judge per question × 4 strategies + test set generation).

## Acceptance Criteria

- [ ] `reports/evaluation_report.md` committed and readable in GitHub
- [ ] Strategy comparison table present with scores for all 4 combinations
- [ ] Cross-check section shows RAG vs context-stuffing win/loss breakdown
- [ ] F43 in `docs/PRD.md` updated from "Not yet run" to ✅ Done




-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.03.01 at 04:23:01.]
