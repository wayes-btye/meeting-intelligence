# [\#66 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/66) `open`: feat: contextual retrieval — prepend document context to chunks before embedding
**Labels**: `enhancement`, `pipeline`, `retrieval`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-23 11:35](https://github.com/wayes-btye/meeting-intelligence/issues/66):

## Background

Anthropic's contextual retrieval research shows a **67% reduction in retrieval failures** (combined with reranking) when each chunk is prepended with a brief document-level context summary before embedding.

**Why this matters for meeting transcripts specifically:** A chunk like "The motion was seconded and passed unanimously" is nearly impossible to retrieve without knowing which meeting it came from, what topic it covers, or who was speaking. Adding document context before embedding makes each chunk self-contained for retrieval.

## Approach

Before embedding each chunk, generate a short context sentence via Claude:

> *"This excerpt is from a Denver City Council meeting (May 2017) discussing zoning ordinance 17-0161. The speaker is Councilwoman Johnson responding to a public comment about parking."*

Prepend this to the chunk text before calling the OpenAI embedding API. The underlying chunk `text` is unchanged — only the embedded representation is enriched.

## Implementation

**1. New function in `src/ingestion/embeddings.py`:**
```python
def generate_chunk_context(chunk: Chunk, meeting_title: str) -> str:
    """Call Claude to generate a 1-2 sentence context for this chunk."""
    ...

def embed_chunks_with_context(chunks, meeting_title) -> list[ChunkWithEmbedding]:
    """Contextualise each chunk before embedding."""
    for chunk in chunks:
        context = generate_chunk_context(chunk, meeting_title)
        text_to_embed = f"{context}\n\n{chunk.text}"
        chunk.embedding = embed(text_to_embed)
    ...
```

**2. New `PipelineConfig` option:** `contextual_retrieval: bool = False` — off by default, opt-in per ingest.

**3. Evaluation:** Run the evaluation framework (Issue #64) before and after to measure actual improvement.

## Cost

~1 extra Claude API call per chunk at ingest time. For 100 chunks: ~$0.10–0.50. Negligible.

## Acceptance Criteria

- [ ] `generate_chunk_context()` implemented and unit tested (mocked)
- [ ] `PipelineConfig` has `contextual_retrieval` toggle
- [ ] Evaluation shows measurable improvement in context recall metric
- [ ] Architecture doc updated with findings




-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.02.24 at 04:20:21.]
