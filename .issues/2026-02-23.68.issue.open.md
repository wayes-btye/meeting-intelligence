# [\#68 Issue](https://github.com/wayes-btye/meeting-intelligence/issues/68) `open`: feat: LiteLLM abstraction for swappable generation and embedding models
**Labels**: `enhancement`, `infrastructure`, `post-mvp`


#### <img src="https://avatars.githubusercontent.com/u/151667361?v=4" width="50">[wayes-btye](https://github.com/wayes-btye) opened issue at [2026-02-23 11:36](https://github.com/wayes-btye/meeting-intelligence/issues/68):

## Context

Currently hardcoded to Claude (generation/extraction/evaluation) and OpenAI text-embedding-3-small (embeddings). This works for the prototype but blocks:

- **Model comparison** — Claude vs GPT-4o vs Llama 3 on answer quality for the same retrieved context
- **Embedding comparison** — OpenAI vs Cohere embed-v3 vs Voyage AI vs sentence-transformers
- **Data-residency deployments** — regulated environments (pharma, healthcare) may require self-hosted models; LiteLLM routes to HuggingFace/Ollama/vLLM with the same call interface

## Approach

Replace direct SDK calls with `litellm.completion()` and `litellm.embedding()`. LiteLLM provides a unified API — it's a thin routing layer, not a framework. The call structure is identical; only the `model` string changes.

**Generation (currently):**
```python
client = anthropic.Anthropic()
response = client.messages.create(model="claude-sonnet-4-6", ...)
```

**With LiteLLM:**
```python
response = litellm.completion(model=settings.generation_model, messages=[...])
# settings.generation_model = "claude-sonnet-4-6" | "gpt-4o" | "ollama/llama3"
```

**Embeddings:**
```python
response = litellm.embedding(model=settings.embedding_model, input=[...])
# settings.embedding_model = "text-embedding-3-small" | "voyage-3" | "cohere/embed-english-v3.0"
```

## Config Changes

```python
# src/config.py
generation_model: str = "claude-sonnet-4-6"
embedding_model: str = "text-embedding-3-small"
```

Both configurable via environment variables — zero code change to switch models.

## Note

This issue is the implementation of both Issue #20 (LLM routing) and Issue #21 (embedding configurability). If those issues are still open, this supersedes them — check before creating worktree.

## Acceptance Criteria

- [ ] All direct `anthropic.Anthropic()` calls replaced with `litellm.completion()`
- [ ] All direct `openai.OpenAI().embeddings` calls replaced with `litellm.embedding()`
- [ ] `generation_model` and `embedding_model` configurable via env vars
- [ ] Existing tests pass (mock at litellm level)
- [ ] README updated with model swap instructions




-------------------------------------------------------------------------------



[Export of Github issue for [wayes-btye/meeting-intelligence](https://github.com/wayes-btye/meeting-intelligence). Generated on 2026.02.25 at 04:21:35.]
