Hi, this is Eric Johnson. It's February 18th, 2021, and this is the engineering key review at GitLab. Um, so I've got number 4 in the agenda, which is a proposal to break up this meeting into 4 department key reviews. So currently this is engineering, um, development, quality, security, and UX. Infrastructure and, um, support do their own key reviews already. I have the reasons Why increase visibility, able to go deeper, um, uh, increase the objectivity with which my reports can manage their groups, allow me more time to focus on new markets, and, uh, allow me to shift into more of a question-asker mode than generating content and answering questions in these meetings. Um, and, uh, but to avoid adding 3 net new meetings to stakeholders' calendars, I propose we do a sort of 2-month rotation. So we'll Month 1, development, quality go. Month 2, security and UX would go. How do people feel about that proposal? I think in the group conversations it's working really well. Um, so I'm, I'm supportive. And, uh, this is the smallest thing. Maybe we need 4 meetings a month. Like it's the biggest department. It's super essential. But you proposed this. I don't— I could see either way, so let's stick with the proposal. Cool, we'll try it and we'll, uh, we can be flexible. I mean, development is larger, maybe they go more frequently or something like that, but we'll see how it, see how it goes. All right, and then I've got number 5, um, which is, uh, we've got, um, R&D overall MR rate and we also have R&D wider MR rate, both as top-level KPIs for engineering. So the difference between them in the simplest sense is that R&D wider MR rate includes both community contributions and community MRs. Um, the problems I see with this are that one, the wider MR rate, uh, the one that includes internal and external MRs, it duplicates the overall MR rate, which is— sorry, sorry, sorry. The wider MR rate should just be external, right? And then overall should be narrow plus wider. Oh yeah, like we say, the wider community, right, right, right. Okay, so there's— I have to check the taxonomy. Lily, can you, uh, Can you confirm Sid's re— that's, that's Sid's reasoning is my understanding as well. Yeah, I believe wider, uh, MR rate just captures community contributions only and no internal. Yeah. And the reason we measure that is that like one of the most likely failure modes is that we lose the community. Yeah. So, er, where it gets goofy is, is that, um, when you look at a specific team within the company, there could be contributions outside of that that aren't community contributions. They would be viewed community contributions by that group, but effectively they're not from outside the company. So that's why we use wider to kind of reflect that, and narrow is very specific to the team. Are you saying that if, uh, someone in plan contributes to verify, it's viewed as wider? Not quite that. Plan, plan and verify are just fine. Uh, it's when you look at, uh, like the development versus as infrastructure. Infrastructure will oftentimes contribute to development work, but it won't be counted as MRs. OK, that's a— that's a potential bit of funkiness that we should talk about separately. I didn't have that in my sort of critique of this, but that doesn't necessarily make intuitive sense to me. So then I think part of my critique of this can be thrown out because it's not as duplicative as I thought. But I still think there's a problem with R&D wider MR rate, which is, um, this thing doesn't really move in part because it's a, it's a rate. So it feels like the way to drive this up is to specifically drive community authors to contribute more than one MR per month. That's how this moves up because it's a, it's a productivity rate like we use internally. And that doesn't necessarily feel like the right thing, cuz there's scenarios in which this goes up and we've actually got less contributions overall and less contributors overall. Wait, wait, wait, wait a second. So you, you're saying that R&D wider MR rate is number MRs per external contributor? Oh my goodness, that should not be the thing. It should be contributions per GitLab team member. So the contrib— those— the, the, the thing above the division is the external ones. The thing below is the number of team members at GitLab. Is that the case, Lily? I'm checking right now, um, I think so. In our numer— merge per team member, so unless we start calling people outside the company team member, then it shouldn't be that. Yeah, just clarifying here, so our numerator is community contributions and then our denominator is, uh, GitLab team members. So it's not per external member. So we— so what we're doing there, Erik, is we're not trying to say how many, how many MRs does someone send if they send something. We're saying how many MRs from external do we get for the size of our organization. Sorry, I have a childhood emergency outside the door. Um, so, so the— maybe explain the context behind this. The context is, as we grow as a company, we should make sure we we keep the community up. Like, the logical thing is for the community to flatline and the size of the org to grow, and before you know it, you've kind of outgrown the wider community. Yeah, and what I'm seeing is we, we created this pretty sophisticated taxonomy with prefixes and postfixes to talk about these things, but in reality, we've only got two of them, and it's— we keep forgetting and we have a hard time discussing this thing. So I'd rather just name them simply some two names for what they are rather than, than using the taxonomy. But also, like, in F, I have this proposal of, like, what if we just tracked as a KPI the percentage of total MRs that come from the community over time? And we would see that drop. I love that, I love that. Let's do that instead. Okay, but the, the thing, the thing why we have this complex thing is because You can game that. You want to game that, you just produce fewer MRs with the engineers at GitLab. So if you drive that really hard and say this is your number one goal, it's very easy to achieve. You just tell all your engineers to produce half. Yeah, so we have, we have different metrics to prevent that from happening. The same way that like support SLAs and SAC kind of buttress one another, um, I think we're, we're robust to that. But simplifying this would make these conversations go. If, if you as our CTO don't even understand them, we went overboard. So I'm supportive. I did understand them and I forgot, and I was reviewing the stuff this morning, I'm like, there's a problem with this, and then you just remind me of the context. So yeah, if I can't hold it in my head— Percentage that come from the community, I love that. It's what all our investors ask about. Let's do that. Okay, cool. So, um, Lily, if you can work with, uh, Mac to make that transition, that would be great. And I'll bold the one that we're talking about, and that's Roman numeral III. Thanks. Uh, I'm on the call, sorry I was a bit late. Uh, timeline, we do have PIs on the raw number of community MRs, and we can make, make the shift. Uh, and wider, confirming from the definition, I think wider only counts for community, and then that's, that's what the definition is. Cool. All right, so, uh, number 6 then, Christopher. Sorry, I was looking up to see if I had the percentage graph because I think we played around with this at one point and had a draft of that, um, probably about 5 months back, if I can remember, Lily. Um, uh, just an FYI, uh, the month of February, if you were looking at any particular metrics, particularly in development at an MR rate, um, uh, we haven't had updates in 4 days. There's apparently a lag issue that's been problematic, uh, for the data team to basically get, uh, updated message— updated metrics, and they're working on that. Great. I'm sorry, Mac, you got the next one. Uh, so yeah, I, I— there's some, some color there on, on the, on mitigation, the, the lag later on. Uh, on to, on to 7. Uh, we continue— as an FYI, in addition to the KPI status I'm sorry, I wanted to just touch on the Postgres replication issue there real quick. Um, I've, I've been trying to get my arms wrapped around it. Um, do we have the right attention to this? This is kind of— Eric, I don't know if you were commenting on, uh, hinting towards this in the last meeting around some of the infrastructure improvements on the product side. I'm just not quite sure whose responsibility it is to focus on getting a handle on some of the constraints we have in replication. Yeah, the— what I was mentioning in the, in the product key review about an hour ago is I think it's sort of like unrelated, um, and so I think the DRI needs to be your kind of data engineering team, but of course there's a dependency on infrastructure because that's where the data is being piped from. They, they do own that data source. Yeah, I'll say for the replication lag on that slave host where— I'm sorry, not the— on the, the, uh, secondary host where, um, uh, the data is being pulled from, like, the infrastructure would be the DRI for that. And so, um, any escalations, but I'll, I'll own those. And I know we have, uh, an action plan for that as far as creating, uh, another dedicated host, um, just for the, the data team to pull from. Okay. I did ask, uh, I saw that issue and I did talk to Craig Gomes a little bit as well on the database side just to see if there's some database improvements. And I'm still trying to figure out, you know, if it's truly just a dedicated computational sort of resource, a server, or if there's actually some, some database tuning that needs to occur. Do you have a sense of that? There's— so I'd say it's, it's 3 different things. Um, it's having a dedicated host that doesn't have conflicting query traffic coming from other, other workloads. Um, there are some tuning performance— or tuning improvements to be made. And then there's also improvements in— and this is where it does maybe relate a little bit to what the topic was in the last review— basically the, the overall demand on the database layer from, um, from.com activity. And like improving those. So it's, it's definitely not just one of those things, um, but one of the, the most specific actions, uh, we're going to take though is separating out and having a dedicated host so that we're just dealing with the profile of the data engineering traffic on there, um, and, and not having conflicting query, um, uh, conflicting queries, uh, affect the ability to, um,, to update the replication. Steve, I definitely want to partner with you on this one because I don't— I think the, the demand on those databases is only going to increase. It's not going down. And I think we need to get— I'm still unclear on, uh, where to focus in to get the biggest bang for the buck. I think if the computational resource dedication, that's going to be a good thing, but I'll probably going to squeeze the balloon and then the next, uh, area will, will unearth itself. Okay, I'll tell you what, I'll put into the infra key review for next week, um, an update on, on this issue. Thank you, Steve. So then back to Mek on 7, or— yes, thank you. Uh, and Rob was in the SAS incident this morning as well. Ryan, just— we have the attention there. Uh, number 7, just to provide, provide an update on previous conversations, uh, we're continuing to improve, uh, defect tracking and against SLOs. There is a first iteration PI that we are experimenting to show, uh, percentage of defects meeting, um, the SLOs. Uh, key findings: S1s are hovering at 80%, S2 at 60%. We've been focused mostly on S1 and S2 at this point, hence why S3 and S4 are lower. And this, this will likely be the case. We are also, in point B, are working on the measurement for average open bugs age. This would give us a whole picture of what's, what's left. If the age goes up or down, if we are cleaning the backlog, the average age should go down as well. There's no PI yet, but I just want to update and ensure that we are on this. It's not, it's not a off track. Uh, number C, Craig, on S2. Yeah, I just— I was looking through the, the charts and I, I noted that there was a spike in mean time to close and just wanted to see if you had any insight into that for us. This is the S2s. The S1s looked fine, so yeah, this is where the point B on age, supplement those charts in the backend helps. Uh, so I haven't seen a dip in age nor the count overall. I, I think, I think it's the latter. We need to dig in a bit deeper in that, and also the data lag, I would like to reevaluate once we have the whole picture, once everything is synced in as well. Uh, Kristy, uh, you have some insights? Yeah, I'm just wondering if part of this could be the fact that we changed the severity across the board for MRs to S2, and so we may have some older bugs in there that hadn't been addressed because they were at a lower severity. Now we've moved them to S2, and maybe that caused a little spike. Uh, that, that could be the case if we did it in a limited fashion. It won't be a huge volume. We also iterated after that to, to P9 priority, uh, since product owns prioritization. So I, I wouldn't account it entirely to that. I mean, this, this isn't the infra key review, but, um, I know that they've gotten backed up on, on those issues, so if some, some good portion of those are infra-created or related, then that might be lifting it as well. I can take the, the, the deeper dig in and then provide an update next time. I think we need extra debug, uh, slicing of the data here. Siddhu, would you like to go to point 8? Yeah, we are now measuring S1, S2, SLO achievement with closed bugs. But if you then look at the, the number of bugs, it's exponential growth, um, and then it will be trivial to, uh, to achieve 100% SLO achievement. If you just look at closed bugs, even though there would be a major problem in the company— 99% of all bugs are overdue— as long as I only close ones that are still within the SLO, I'll have great achievement. So I think we shouldn't be looking at the closed bugs. I think we should be looking at open bugs, the entire population, or percentage of those is within the SLO time. I think we're doing it the wrong way. Thanks for the— thanks for the feedback, Sid. Hence why we wanted to have the average age to, to measure what's outside in the open. We can make this iteration to measure— also measure, uh, focus on the, the age of all opened, uh, including open bugs. This is all something we— I have discussed with Christopher in the next iteration as well, and we're happy— more than happy to adjust. Uh, so the, the key— yeah, go ahead. So average age would get closer to it. It's not what I'm proposing. What I'm proposing is of the open bugs, what percentage is outside of SLO. So display it as a percentage. You do now, just do it about the open bugs, not the closed ones. Ah, got it. Okay, the exceeding SLO for open bugs. Yeah, or open bugs that are within SLO. So you have a chart that should go up and to the right. Like everything else. Sounds great, we can take it to the, uh, the next, uh, uh, data metrics work stream to deliver this. Cool, thanks. It's a pretty— yeah, a little tricky, Mac. You'll have to figure out how to, um, because we like to be able to have charts that we can historically reconstruct if we need to. So when tickets close out, you need to go through their history to figure out at this time when it was open, did it breach the SLO or not? That's a good point. This might be much harder computationally, um, so I, I totally respect if we can do it for that reason. Uh, cool. Nine, Craig? Yeah, I just wanted to ask the team, like, I went through all the, the key meeting metrics, everything looked in line with prior periods and look good. Is there anything the team wants to call out, especially that we should be watching? Yeah, I'll call out sus, um, so the good news is in Q4 we had our smallest decline over several quarters, so we only went down by a tenth of a point. Um, the quarter previous was 0.6 or 6 tenths of a point, and the quarter before that was a full point, so we see this as an improvement even though it was still a decline, but it's still a decline. Obviously we want this actually tracking in an upward direction. We also don't have enough data to know whether or not this is an actual real trend up. So I'm optimistic. I think this is a good thing. We have had a much keener focus on sus over the past several quarters. So that's why I think, okay, the work that we've done, I think actually is catching up and, and getting noticed in sus, but we got to keep an eye on it. We can't— we cannot assume that that's the case. Yeah, and the bug discussion above, um, just kind of points out that like, uh, we have an underlying, um, problem right now in our metrics measurement. So if we change the measurements to reflect that, then hopefully we're in good shape. If we don't and we flatline and address it so that we flatline open S1s and S2s, you will see a temporary jump in, in above SLOs as we clear out that backlog over that period of time. And I have point C, which is, uh, similar to infrastructure, we need to get more security work prioritized. We're hearing that from the team, but neither that problem nor that activity is sort of currently reflected in our security metrics. So we have some work to do, um, long term to, to make sure the— we see things like that in the metrics and the measurements that we're making. Uh, so back to you, Sid Tan. Yeah, the narrow MR rate seems, um, significantly below target, and maybe I, I hope that it would bounce back from December. I think it bounced back but not back on target. Any, any context there? What's, what's going on? Yeah, so, um, uh, with Family and Friends Days, we actually had some heavier vacation days, um, in January than we historically have. One thing to note is that, um, we are actually at a higher MR rate. If you look at— if you go back the last 18 months, we're actually at a higher NRR MR rate than we were back in, uh, each month this year. So if you compare October to October, November to November, and January— I'm sorry, October, November, December, and January, uh, comparatively to last year, which you'll find is we're between a half point and a 1.5, uh, MR rate above, uh, where we were in the month of previous year. That's, that's great context. Thank you, Christopher. Good work. Yeah, so the expectation is, is that, uh, February is a short month. Uh, we are at, I think, 16 workdays with Friends and Family Day and other things. Obviously, us having power outages in Texas doesn't help things either for the folks who are working in Texas. But hopefully the rest of the team is being effective. I was hoping to see a better result right now, but with 4 or 5 days, particularly around release week, that's usually when we see— do see a little bit higher activity. So that's not accounted for yet. But, you know, March is— March is when I'm expecting to kind of see a real rebound, much like we did last year. Awesome, thanks. The other context I'll give is we now do time series targets. So when we change the target, you'll see that reflected in the line. So if we were to look back historically here, the goal here was actually lower and Christopher was ambitious and we kept raising and we kept meeting that. So it should stair-step here and we could go back and reconstruct that if we really wanted to. And then ignoring the sort of the, the seasonal dip here, um, we raised it to I think 11 and then we realized we're kind of hitting that point of diminishing returns and the right thing to do business wise, and this is in our FY22 direction, is hold the line at productivity but start to raise other things related to quality, security, availability, and, and whatnot. Um, so that's kind of why you're seeing this bump, is we raised it and we realized, okay, that's not the— we shouldn't raise it anymore, and we brought it back down to 10. So 10 will be the target going forward. I'm going to try to get better a lot of other things while preventing this from dipping. Cool. And I just want to call out that it's not necess— like, higher MR rates should also help to address security and quality and other things because you're more productive, so you can fix more things. So it's not necessarily opposite, but I, I, I agree with, uh, let's hold the line. 10 is, 10 is a great number, and, uh, and focus on other, uh, indicators to improve. That makes a ton of sense. Cool, well said. All right, that's it for the agenda. Anyone want to vocalize anything else? Great. Well, thanks everybody, and I'm going to go check on my 4-year-old and see if she got what— whatever she needed. So cheers and talk soon. Thanks, Eric.